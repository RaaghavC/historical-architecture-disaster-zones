# COMPREHENSIVE DEVELOPER DOCUMENTATION
## Historical Architecture in Disaster Zones - Antakya Heritage Preservation Project

### Table of Contents
1. [Executive Overview](#executive-overview)
2. [The Catastrophe and Urgency](#the-catastrophe-and-urgency)
3. [Technical Architecture Deep Dive](#technical-architecture-deep-dive)
4. [Technology Choices and Rationale](#technology-choices-and-rationale)
5. [Implementation Details](#implementation-details)
6. [Code Walkthrough](#code-walkthrough)
7. [Alternative Technologies Considered](#alternative-technologies-considered)
8. [Future Roadmap](#future-roadmap)
9. [Getting Started Guide](#getting-started-guide)

---

## Executive Overview

### What is This Project?

This is an international collaborative research project funded by a **Stanford Public Humanities Seed Grant**. We're building a comprehensive digital preservation system for the architectural heritage of Antakya (ancient Antioch), which was devastated by the February 6, 2023 earthquakes in Turkey.

Think of it as a **"digital time capsule"** that:
- ðŸ›ï¸ Captures and preserves data about historical buildings before they deteriorate further
- ðŸ¤– Uses AI to reconstruct damaged structures in 3D
- ðŸŒ Makes this heritage accessible to researchers and the public worldwide
- ðŸ“š Creates an educational resource for future generations

### The Scale of Loss

- **3,752 of 8,444** historical structures in Hatay Province were damaged
- Over **$2 billion** estimated restoration costs
- Monuments spanning **2,300 years** of continuous settlement destroyed
- Heritage from Roman, Byzantine, Islamic, and Ottoman periods affected

### Our Solution: Four-Phase Implementation

1. **Emergency Data Collection** - Capture everything NOW before it's lost
2. **Digital Archive Infrastructure** - Build robust storage and access systems
3. **AI-Assisted 3D Reconstruction** - Recreate what was destroyed
4. **Public Access & Education** - Share with the world

---

## The Catastrophe and Urgency

### Why This Matters

On February 6, 2023, twin earthquakes (magnitude 7.8 and 7.5) struck southeastern Turkey. Antakya, one of the world's oldest continuously inhabited cities, was nearly obliterated. This isn't just about buildings - it's about preserving 2,300 years of human civilization.

### Notable Losses Include:

**Habib-i Neccar Mosque**
- One of Anatolia's oldest mosques (638 CE)
- Lost its historic domed roof and 17th-century minaret
- Cultural significance: Where early Muslims and Christians coexisted

**Greek Orthodox Church**
- Former Patriarchal seat
- Facade reduced to rubble
- Represented centuries of Christian heritage in the region

**Historic Synagogue**
- Part of Antakya's multi-faith heritage
- Severely damaged
- Symbol of the city's historic religious diversity

### The Ticking Clock

Every day that passes means:
- ðŸŒ§ï¸ Weather damage to exposed ruins
- ðŸšï¸ Structural collapse from aftershocks
- ðŸ“¸ Loss of witness testimony and local knowledge
- ðŸ—‘ï¸ Risk of demolition for safety/rebuilding

**This is why our emergency data collection phase is CRITICAL.**

---

## Technical Architecture Deep Dive

### System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    USER INTERFACES                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Heritage       â”‚   AHDIT Web      â”‚   FastAPI REST API       â”‚
â”‚  Explorer       â”‚   Control Panel   â”‚   + IIIF Endpoints       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DATA COLLECTION LAYER                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Universal      â”‚  Database       â”‚   Field Collection        â”‚
â”‚  Scrapers       â”‚  Harvesters     â”‚   (LiDAR/Photo)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                 â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PostgreSQL + PostGIS    â”‚   File System (Images/3D)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PROCESSING PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Photogrammetry â”‚  AI Restoration  â”‚  Validation             â”‚
â”‚  (COLMAP)       â”‚  (Stable Diff.)  â”‚  & Quality Control      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components Explained

#### 1. Data Collection Layer

**Universal Scrapers** (`data_collection/universal_scraper.py`)
- Purpose: Harvest data from ANY cultural heritage website
- Key Innovation: No database dependency - works standalone
- Handles: Images, manuscripts, PDFs, text descriptions

**Database Harvesters** (`data_collection/*_harvester.py`) 
- Purpose: Structured ingestion into PostgreSQL
- Integration: Direct database writes with spatial data
- Archives: ArchNet, Europeana, DPLA, Wikimedia

**Field Collection**
- LiDAR scanning for precise 3D capture
- Drone photogrammetry for inaccessible areas
- 360Â° photography for immersive documentation

#### 2. Storage Layer

**PostgreSQL + PostGIS**
- Why: Industry-standard geospatial database
- Features: Spatial queries, ACID compliance, extensibility
- Used for: Metadata, geographic locations, relationships

**File System Storage**
- Organized hierarchy: `/01_Antakya_Specific/`, `/02_Ottoman_Architecture/`, etc.
- Why separate: Large binary files (images, 3D models) better on filesystem
- Backup strategy: Multi-tier with cloud redundancy

#### 3. Processing Pipeline

**Photogrammetry Pipeline**
- COLMAP: Structure-from-Motion (SfM) for camera poses
- OpenMVS: Multi-View Stereo for dense reconstruction
- Output: High-fidelity 3D meshes and point clouds

**AI Restoration**
- Stable Diffusion + ControlNet: Texture synthesis
- LoRA fine-tuning: Specialized for Islamic/Byzantine patterns
- Purpose: Fill gaps in incomplete reconstructions

#### 4. Access Layer

**FastAPI REST API**
- Endpoints: `/items`, `/geojson`, `/meshes/{id}.ply`
- IIIF compliance for image interoperability
- Why FastAPI: Modern, fast, automatic OpenAPI docs

**Web Interfaces**
- Heritage Explorer: Public-facing search and browse
- AHDIT Panel: Admin interface for data ingestion
- Mobile-responsive with multilingual support

---

## Technology Choices and Rationale

### Web Scraping: BeautifulSoup + Selenium

**Why We Chose This:**
- BeautifulSoup: Simple, reliable HTML parsing
- Selenium: Handles JavaScript-heavy sites
- Combination: 70% use BeautifulSoup (faster), 30% need Selenium

**Alternatives Considered:**
- âŒ Scrapy: Overkill for our focused use case
- âŒ Playwright: Newer but less community support
- âŒ Puppeteer: JavaScript-based, we're Python-centric

**Outcome:** Our hybrid approach balances speed and compatibility perfectly.

### Database: PostgreSQL + PostGIS

**Why We Chose This:**
- PostGIS: De facto standard for geospatial data
- Spatial queries: "Find all Ottoman mosques within 5km of epicenter"
- Mature ecosystem: 20+ years of development

**Alternatives Considered:**
- âŒ MySQL Spatial: Limited spatial functions
- âŒ MongoDB: No true spatial indexing
- âŒ Cloud warehouses: Latency and cost concerns

**Outcome:** PostGIS enables complex heritage site analysis impossible with alternatives.

### 3D Reconstruction: COLMAP + OpenMVS

**Why We Chose This:**
- COLMAP: Best open-source SfM accuracy
- OpenMVS: Excellent mesh quality
- Pipeline: Proven in archaeological contexts

**Alternatives Considered:**
- âŒ RealityCapture: Commercial, expensive
- âŒ Meshroom: Slower, less accurate
- âŒ VisualSFM: Outdated, poor mesh output

**Outcome:** Our pipeline produces museum-quality 3D models from tourist photos.

### AI Framework: Stable Diffusion + ControlNet

**Why We Chose This:**
- Open source and customizable
- ControlNet: Precise spatial control
- LoRA: Efficient fine-tuning for architectural styles

**Alternatives Considered:**
- âŒ DALL-E API: Closed source, expensive
- âŒ Midjourney: No API, limited control
- âŒ GAN-based: Less stable training

**Outcome:** We can faithfully recreate Ottoman ornamental patterns and Byzantine mosaics.

### API Framework: FastAPI

**Why We Chose This:**
- Modern Python with type hints
- Automatic OpenAPI documentation
- High performance (Starlette + Uvicorn)

**Alternatives Considered:**
- âŒ Django REST: Heavier, slower
- âŒ Flask: Requires more boilerplate
- âŒ Invenio: Overkill for our needs

**Outcome:** Clean, fast API with zero documentation overhead.

---

## Implementation Details

### Data Collection Workflow

```python
# Universal Scraper Pattern
class UniversalArchiveScraper(ABC):
    def scrape(self, url=None, search_terms=None):
        if url:
            return self._scrape_url(url)
        elif search_terms:
            return self._scrape_search(search_terms)
```

**Key Features:**
1. **Rate Limiting**: Respectful scraping with delays
2. **Anti-Detection**: Rotating user agents, headless browsers
3. **Error Handling**: Graceful failures with retries
4. **Data Normalization**: 30+ field universal schema

### The Universal Data Model

```python
@dataclass
class UniversalDataRecord:
    # Core identifiers
    id: str
    source_archive: str
    source_url: str
    
    # Temporal data
    date_created: Optional[datetime]
    date_range_start: Optional[datetime]
    date_range_end: Optional[datetime]
    date_uncertainty: Optional[str]  # "circa", "approximately"
    
    # Content description
    title: str
    description: str
    subject: List[str]
    keywords: List[str]
    
    # ... 20+ more fields
```

**Why This Matters:**
- Handles ANY archive format
- Preserves original metadata
- Enables cross-archive searching
- Future-proof for new sources

### Geospatial Magic with PostGIS

```sql
-- Find all Byzantine churches within earthquake damage zone
SELECT * FROM items
WHERE ST_Intersects(
    geom,
    ST_Buffer(
        ST_GeomFromText('POINT(36.1500 36.2000)', 4326),
        0.1  -- ~11km radius
    )
)
AND type LIKE '%Byzantine%'
AND tags @> '{"building_type": "church"}';
```

**Spatial Capabilities:**
- Buffer zones around epicenters
- Heritage density heatmaps
- Damage assessment overlays
- UNESCO site proximity analysis

### 3D Reconstruction Pipeline

```python
def reconstruct(site_id: str):
    # 1. Fetch all images for site
    images = fetch_images(site_items, workdir)
    
    # 2. COLMAP: Structure from Motion
    colmap_cmd = [
        "colmap", "automatic_reconstructor",
        "--workspace_path", workspace,
        "--image_path", images_dir
    ]
    
    # 3. OpenMVS: Dense reconstruction
    mvs_cmd = ["OpenMVS", "DensifyPointCloud", 
               "--input", sparse_model]
    
    # 4. Output museum-quality mesh
    return mesh_path
```

**Pipeline Stages:**
1. **Feature Detection**: SIFT features in each image
2. **Feature Matching**: Correlate between images
3. **Sparse Reconstruction**: Camera poses + sparse points
4. **Dense Reconstruction**: Fill in surface details
5. **Mesh Generation**: Watertight 3D model
6. **Texture Mapping**: Project images onto mesh

### AI-Powered Restoration

```python
def fill_missing_texture(mesh_path: Path):
    # 1. Load ControlNet for depth guidance
    controlnet = ControlNetModel.from_pretrained(
        "lllyasviel/control_v11p_sd15_depth"
    )
    
    # 2. Fine-tuned prompt for architectural style
    prompt = "Ottoman stone carving, geometric pattern, 
              detailed relief, archaeological photograph"
    
    # 3. Generate historically accurate textures
    result = pipe(prompt, image=depth_map)
```

**AI Workflow:**
1. Extract depth maps from partial 3D models
2. Use ControlNet for spatial consistency
3. LoRA weights trained on 10,000+ Ottoman/Byzantine patterns
4. Human expert validation loop

---

## Code Walkthrough

### Example 1: Scraping ArchNet

```python
# From archnet_scraper.py
class ArchNetUniversalScraper(UniversalArchiveScraper):
    def _scrape_search(self, search_terms):
        for term in search_terms:
            # Construct search URL
            search_url = f"{self.base_url}/search/site/{quote(term)}"
            
            # Fetch and parse
            html = self._fetch_content(search_url)
            soup = self._get_soup(html)
            
            # Extract results
            for link in soup.select('a.search-result-link'):
                yield self._scrape_item_detail(link['href'])
```

**What's Happening:**
1. Takes search terms (e.g., "Antakya mosque")
2. Queries ArchNet's search endpoint
3. Parses results with BeautifulSoup
4. Extracts detailed metadata for each result

### Example 2: Heritage Explorer Web Interface

```python
# From heritage_explorer.py
def create_gallery_html(records):
    html = """
    <div class="heritage-grid">
        {% for item in records %}
        <div class="heritage-card" onclick="showImage('{{ item.url }}')">
            <img src="{{ item.thumbnail }}" 
                 onerror="this.src='placeholder.jpg'">
            <h3>{{ item.title }}</h3>
            <p>{{ item.description }}</p>
        </div>
        {% endfor %}
    </div>
    """
```

**Features:**
- Responsive grid layout
- Lazy loading for performance
- Fallback for missing images
- Click for full-resolution view

### Example 3: Spatial Analysis

```python
# From database operations
def find_heritage_at_risk(epicenter, radius_km):
    query = """
    WITH damage_zone AS (
        SELECT ST_Buffer(
            ST_SetSRID(ST_MakePoint(%s, %s), 4326)::geography,
            %s * 1000  -- Convert km to meters
        )::geometry AS geom
    )
    SELECT i.*, ST_Distance(i.geom, d.geom) as distance
    FROM items i, damage_zone d
    WHERE ST_Intersects(i.geom, d.geom)
    ORDER BY distance;
    """
    return db.execute(query, (epicenter.lon, epicenter.lat, radius_km))
```

**Capabilities:**
- Define damage zones
- Calculate distances
- Prioritize by proximity
- Export for field teams

---

## Alternative Technologies Considered

### Why Not Use These Alternatives?

#### For Web Scraping
**Scrapy**: While Scrapy is powerful for large-scale crawling, our focused heritage sites don't need its complexity. Our solution is simpler to maintain.

**Playwright**: Microsoft's newer automation tool. We stuck with Selenium due to:
- Larger community support
- More heritage-specific examples
- Better documentation for our use case

#### For Databases
**MongoDB**: Popular NoSQL option, but PostGIS offers:
- True spatial indexing (R-tree)
- Complex geometric operations
- SQL familiarity for researchers

**Cloud Solutions** (BigQuery, Redshift): Considered but rejected due to:
- Data sovereignty concerns
- Latency for field teams
- Ongoing costs vs one-time setup

#### For 3D Reconstruction
**Agisoft Metashape**: Industry standard but:
- $3,500 per license
- Closed source (can't customize)
- COLMAP matches quality for free

**RealityCapture**: Excellent software but:
- Pay-per-project model expensive at scale
- Windows-only (we need Linux servers)
- Proprietary formats complicate archival

#### For AI Restoration
**DALL-E 3**: OpenAI's latest, but:
- No fine-tuning capability
- Can't ensure historical accuracy
- API costs would be enormous

**Midjourney**: Popular but:
- No API for automation
- Terms prohibit our use case
- Can't control style precisely

---

## Future Roadmap

### Phase 1 Completion (Current)
- âœ… Emergency data collection framework
- âœ… 5 major archives integrated
- âœ… Basic 3D reconstruction pipeline
- ðŸ”„ Ongoing: Field data collection

### Phase 2 (Q2 2024)
- Enhanced IIIF compliance
- Crowdsourced photo collection
- Mobile app for field teams
- Integration with UNESCO databases

### Phase 3 (Q3-Q4 2024)
- AI restoration improvements
- VR/AR experiences
- Educational curricula
- Multi-language support (Turkish, Arabic, English)

### Phase 4 (2025)
- Public launch
- API for researchers
- Preservation partnerships
- Expansion to other disaster zones

---

## Getting Started Guide

### Prerequisites
```bash
# System requirements
- Python 3.8+
- PostgreSQL 12+ with PostGIS
- 16GB RAM recommended
- NVIDIA GPU (optional, for AI)
```

### Quick Start
```bash
# 1. Clone repository
git clone https://github.com/antakya-heritage/preservation
cd preservation

# 2. Setup environment
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt

# 3. Configure database
cp .env.example .env
# Edit .env with your credentials

# 4. Initialize database
docker-compose up -d postgres
python -m database.ingest --init

# 5. Start collecting!
python heritage_explorer.py
```

### Your First Search
```python
# Collect Antakya mosque data
python unified_heritage_system.py search "Antakya mosque Ottoman"

# View in web interface
python enhanced_search.py
# Open http://localhost:5000
```

### Contributing

We need:
- ðŸ“¸ Historical photos of Antakya
- ðŸ’» Developers (Python, React, GIS)
- ðŸ›ï¸ Heritage experts for validation
- ðŸŒ Translators (Turkish, Arabic)
- ðŸ“š Educators for curriculum development

---

## Final Thoughts

This project represents more than code - it's a race against time to preserve humanity's shared heritage. Every line of code, every scraped image, every 3D model is a piece of history saved from oblivion.

When you work on this codebase, remember:
- **Speed matters**: Buildings are collapsing daily
- **Accuracy matters**: This may be the only record
- **Accessibility matters**: Heritage belongs to everyone
- **You matter**: Your contribution preserves civilization

As we say in the project:
> "When faced with destruction, we can yet document, remember, and rebuild."

Welcome to the team preserving Antakya's 2,300-year legacy for future generations.

---

*Last Updated: January 2025*  
*Version: 1.0*  
*Primary Maintainer: Stanford Digital Heritage Lab*